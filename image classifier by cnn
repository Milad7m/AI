import argparse
import tensorflow as tf 
import input_data 
Define a function to parse the input arguments:def build_arg_parser():  
parser = argparse.ArgumentParser(description='Build a CNN classifier \   
using MNIST data')    
parser.add_argument('--input-dir', dest='input_dir', type=str, 
default='./mnist_data', help='Directory for storing data')    
return parser Define a function to create values for weights in each layer:def get_weights(shape):  
data = tf.truncated_normal(shape, stddev=0.1)  
return tf.Variable(data) 
Define a function to create values for biases in each layer:def get_biases(shape):  
data = tf.constant(0.1, shape=shape)    
return tf.Variable(data) Define a function to create a layer based on the input shape:def create_layer(shape):   
# Get the weights and biases    
W = get_weights(shape)    
b = get_biases([shape[-1]])   
return W, b Define a function to perform 2D-convolution:def convolution_2d(x, W): 
return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],        
padding='SAME') Define a function to perform a 2x2 max pooling operation:def max_pooling(x): 
return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],           
strides=[1, 2, 2, 1], padding='SAME') 
Define the main function and parse the input arguments:if __name__ == '__main__':    
args = build_arg_parser().parse_args() 
Extract the MNIST image data:  
# Get the MNIST data   
mnist = input_data.read_data_sets(args.input_dir, one_hot=True) 
Create the input layer with 784 neurons:  
# The images are 28x28, so create the input layer     
# with 784 neurons (28x28=784)     
x = tf.placeholder(tf.float32, [None, 784]) 
We will be using convolutional neural networks that take advantage of the 2D structure of images. 
So let's reshape x into a 4D tensor where the second and third dimensions specify the image dimensions:   
# Reshape 'x' into a 4D tensor    
x_image = tf.reshape(x, [-1, 28, 28, 1]) 
Create the first convolutional layer that will extract 32 features for each 5x5 patch in the image:  
# Define the first convolutional layer 
W_conv1, b_conv1 = create_layer([5, 5, 1, 32]) 
Convolve the image with weight tensor computed in the previous step, and then add the bias tensor to it. 
We then need to apply the Rectified Linear Unit (ReLU) function to the output:  
# Convolve the image with weight tensor, add the      
# bias, and then apply the ReLU function     
h_conv1 = tf.nn.relu(convolution_2d(x_image, W_conv1) + b_conv1) 
Apply the 2x2 max pooling operator to the output of the previous step:   
# Apply the max pooling operator   
h_pool1 = max_pooling(h_conv1) Create the second convolutional layer to compute 64 features for each 5x5 patch:  
# Define the second convolutional layer   
W_conv2, b_conv2 = create_layer([5, 5, 32, 64]) 
Convolve the output of the previous layer with weight tensor computed in the previous step, and then add the bias tensor to it.
We then need to apply the Rectified Linear Unit (ReLU) function to the output:   
# Convolve the output of previous layer with the    
# weight tensor, add the bias, and then apply  
# the ReLU function 
h_conv2 = tf.nn.relu(convolution_2d(h_pool1, W_conv2) + b_conv2) 
Apply the 2x2 max pooling operator to the output of the previous step: 
# Apply the max pooling operator    
h_pool2 = max_pooling(h_conv2) 
The image size is now reduced to 7x7. Create a fully connected layer with 1024 neurons. 
# Define the fully connected layer   
W_fc1, b_fc1 = create_layer([7 * 7 * 64, 1024]) Reshape the output of the previous layer:  
# Reshape the output of the previous layer  
h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) 
Multiply the output of the previous layer with the weight tensor of the fully connected layer, 
and then add the bias tensor to it. We then apply the Rectified Linear Unit (ReLU) function to the output:  
# Multiply the output of previous layer by the    
# weight tensor, add the bias, and then apply    
# the ReLU function   
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) In order to reduce overfitting, we need to create a dropout layer. 
Let's create a TensorFlow placeholder for the probability values that specify the probability of a neuron's output being kept during dropout:  
# Define the dropout layer using a probability placeholder    
# for all the neurons    
keep_prob = tf.placeholder(tf.float32)    
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
Define the readout layer with 10 output neurons corresponding to 10 classes in our dataset.
Compute the output: 
# Define the readout layer (output layer)   
W_fc2, b_fc2 = create_layer([1024, 10])   
y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 Define the loss function and optimizer function:  
# Define the entropy loss and the optimizer   
y_loss = tf.placeholder(tf.float32, [None, 10])    
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_loss))   
optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss) Define how the accuracy should be computed:   
# Define the accuracy computation    
predicted = tf.equal(tf.argmax(y_conv, 1),
tf.argmax(y_loss, 1))    
accuracy = tf.reduce_mean(tf.cast(predicted, tf.float32)) 
Create and run a session after initializing the variables:   
# Create and run a session  
sess = tf.InteractiveSession()  
init = tf.initialize_all_variables()     
sess.run(init) Start the training process:   
# Start training    
num_iterations = 21000    
batch_size = 75    
print('\nTraining the model....')   
for i in range(num_iterations):      
# Get the next batch of images       
batch = mnist.train.next_batch(batch_size) 
Print the accuracy progress every 50 iterations:   
# Print progress    
if i % 50 == 0:    
cur_accuracy = accuracy.eval(feed_dict = {          
x: batch[0], y_loss: batch[1], keep_prob: 1.0})      
print('Iteration', i, ', Accuracy =', cur_accuracy) Run the optimizer on the current batch:  
# Train on the current batch    
optimizer.run(feed_dict = {x: batch[0], y_loss: batch[1], keep_prob: 0.5}) 
Once the training process is over, compute the accuracy using the test dataset:   
# Compute accuracy using test data   
print('Test accuracy =', 
accuracy.eval(feed_dict = {             x: mnist.test.images, 
y_loss: mnist.test.labels,              keep_prob: 1.0})) 
enjoy your classifier by great milad ! 
